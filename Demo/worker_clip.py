from PIL import Image
import torch
from dataclasses import dataclass
from torch.utils.data import Dataset
from pathlib import Path
from typing import List, Tuple, Dict
import os
import random
import cv2
import json
from transformers import LlavaForConditionalGeneration, LlavaProcessor,AutoProcessor
import torch
from peft import peft_model,PeftModel
from stage3_data import LlavaDatasetStage3, TrainLlavaModelCollator, LlavaForConditionalGenerationClip
import matplotlib.pyplot as plt
import zmq
import base64
import numpy as np
import threading
import time
from collections import deque
import re
import hashlib
import json

raw_model_name_or_path = "model/stage1_model"
peft_model_name_or_path1 = "model/checkpoint-12625"
peft_model_name_or_path2 = "model/checkpoint-1482"
model = LlavaForConditionalGenerationClip.from_pretrained(raw_model_name_or_path,device_map="cuda:0", torch_dtype=torch.bfloat16)
model_with_lora1 = PeftModel.from_pretrained(model, peft_model_name_or_path1, adapter_name="peft_v1")
merged_model = model_with_lora1.merge_and_unload()
model_with_lora2 = PeftModel.from_pretrained(merged_model, peft_model_name_or_path2)

processor = LlavaProcessor.from_pretrained(raw_model_name_or_path)
model_with_lora2.eval()

data_root_dir = "dataset"
test_dataset = LlavaDatasetStage3(data_root_dir,"test")
timestamp = time.strftime("%m%d_%M%S")

def natural_sort_key(filename):
    # æå– filename ä¸­çš„æ•°å­—éƒ¨åˆ†ç”¨äºæ’åº
    return [int(s) if s.isdigit() else s for s in re.split(r'(\d+)', filename)]

def sample_clips(clip_path: Path, num_frames: int, split_num: int):
    images_path = [clip_path.joinpath(image_file) for image_file in sorted(os.listdir(clip_path),key=natural_sort_key) if image_file.endswith(".jpg")]
    total_frames = len(images_path[split_num:])
    actual_frames = min(total_frames, num_frames)
    # print(f"actual_frames:{actual_frames}")
    interval = total_frames // actual_frames
    sampled_frames = []
    for i in range(actual_frames):
        frame_index = i * interval
        raw_image = Image.open(images_path[split_num + frame_index])
        sampled_frames.append(raw_image) 

    return  sampled_frames, len(sampled_frames)



def show_sampled_frames(frame_list):
    num_frames = len(frame_list)
    cols = min(num_frames, 3)  # æ¯è¡Œæœ€å¤š5å¼ 
    rows = (num_frames + cols - 1) // cols

    plt.figure(figsize=(15, 3 * rows))

    for idx, frame in enumerate(frame_list):
        plt.subplot(rows, cols, idx + 1)
        plt.imshow(frame)
        plt.axis('off')
        plt.title(f"Frame {idx}")

    plt.tight_layout()
    plt.show()
    

def build_qaclip(model, processor: LlavaProcessor, q_text: str, frames: list, summary: str):
    frame_num = len(frames)
    frame_list = [Image.fromarray(cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)) for raw_image in frames]
    print(f"history:",summary)
    additional_descriptions = "<history>" + summary + "</history>" + "<clip>" + "<image><sep>"*(frame_num - 1) + "<image>" + "</clip>"
    q_text = additional_descriptions + q_text
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": q_text},
    ]
    prompt = processor.tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    processor.patch_size = 14
    inputs = processor(frame_list,prompt, return_tensors="pt")
    for tk in inputs.keys():
        inputs[tk] = inputs[tk].to(model.device)
    generate_ids = model.generate(**inputs,max_new_tokens=2000)
    
    generate_ids = [
        oid[len(iids):] for oid, iids in zip(generate_ids, inputs.input_ids)
    ]
    
    gen_text = processor.batch_decode(generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]
    return gen_text

############################## å®ç°é€»è¾‘ ##################################

# åˆå§‹åŒ– ZeroMQ ä¸Šä¸‹æ–‡
ctx = zmq.Context()

# åˆå§‹åŒ–å‘å¸ƒè€…
publisher = ctx.socket(zmq.PUB)
publisher.bind("tcp://*:5575")  # å‘å¸ƒåˆ°ç«¯å£5575ï¼Œä¸app.pyä¸­çš„é…ç½®åŒ¹é…

# è®¢é˜…å›¾åƒå¸§
frame_sub = ctx.socket(zmq.SUB)
frame_sub.connect("tcp://localhost:5560")
frame_sub.setsockopt(zmq.SUBSCRIBE, b"")

# è®¢é˜…æ‘˜è¦
summary_sub = ctx.socket(zmq.SUB)
summary_sub.connect("tcp://localhost:5563")  # ä¿®æ”¹ä¸ºä¸ worker_summary.py ä¸­çš„å‘å¸ƒç«¯å£ä¸€è‡´
summary_sub.setsockopt(zmq.SUBSCRIBE, b"")
print("ğŸ”” å·²è®¢é˜…æ‘˜è¦ç«¯å£ 5563")

# å…¨å±€ç¼“å­˜
frame_buffer = deque(maxlen=8)  # ç¼“å­˜æœ€è¿‘ 8 å¸§
frame_lock = threading.Lock()

latest_summary = None
summary_lock = threading.Lock()

# æ¥æ”¶å›¾åƒå¸§çš„çº¿ç¨‹å‡½æ•°
def receive_frames():
    global frame_buffer, current_video_info
    print("ğŸ” å¼€å§‹æ¥æ”¶å¸§æ•°æ®...")
    while True:
        try:
            msg = frame_sub.recv_json()
            print("ğŸ“¥ æ”¶åˆ°å¸§æ•°æ®")
            jpg = base64.b64decode(msg["frame"])
            img = cv2.imdecode(np.frombuffer(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)
            print(f"ğŸ–¼ï¸ è§£ç å¸§: {img.shape if img is not None else 'None'}")

            video_meta = {
                "filename": msg.get("video_file", "unknown_video"),
                "index": msg.get("video_index", 0),
                "total": msg.get("total_videos", 1),
                "timestamp": msg.get("timestamp", time.time()),
                "resolution": f"{msg.get('frame_width', 0)}x{msg.get('frame_height', 0)}"
            }

            with frame_lock:
                if img is not None:  # ç¡®ä¿å›¾åƒæœ‰æ•ˆ
                    frame_buffer.append(img)  # å¿…é¡»ä¿ç•™çš„å¸§ç¼“å­˜æ“ä½œ
                    current_video_info = video_meta
            time.sleep(0.5)
        except Exception as e:
            print(f"âŒ æ¥æ”¶å¸§æ—¶å‡ºé”™: {e}")
            time.sleep(1)  # å‡ºé”™æ—¶ç­‰å¾…1ç§’å†é‡è¯•

# æ¥æ”¶æ‘˜è¦çš„çº¿ç¨‹å‡½æ•°
def receive_summary():
    global latest_summary
    print("ğŸ” å¼€å§‹æ¥æ”¶æ‘˜è¦æ•°æ®...")
    while True:
        try:
            msg = summary_sub.recv_json()
            print(f"ğŸ“ æ”¶åˆ°æ‘˜è¦ (åŸå§‹æ•°æ®): {msg}")  # è°ƒè¯•ç”¨
            
            # å¤„ç† summaryï¼ˆå…¼å®¹ str å’Œ listï¼‰
            if isinstance(msg["summary"], list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œåˆå¹¶æˆå­—ç¬¦ä¸²
                summary_text = " ".join(msg["summary"])
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                summary_text = msg["summary"]
            
            # æå–æ‘˜è¦å†…å®¹ï¼ˆå»æ‰å¯èƒ½çš„å‰ç¼€ï¼‰
            if ":" in summary_text:
                latest_summary = summary_text.partition(":")[2].strip()
            else:
                latest_summary = summary_text.strip()
            
            print(f"ğŸ“ å¤„ç†åæ‘˜è¦: {latest_summary}")
        except Exception as e:
            print(f"âŒ æ¥æ”¶æ‘˜è¦æ—¶å‡ºé”™: {e}")

def hash_frames(frames):
    md5 = hashlib.md5()
    for f in frames:
        md5.update(f.tobytes())
    return md5.hexdigest()

def hash_text(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()


# å¯åŠ¨ä¿¡æ¯
print("ğŸš€ CLIP Worker å¯åŠ¨æˆåŠŸï¼")
print(f"ğŸ“¡ ç›‘å¬ç«¯å£: 5560 (å¸§), 5558 (æ‘˜è¦)")
print(f"ğŸ“¤ å‘å¸ƒç«¯å£: 5575")
print("ğŸ”„ ç­‰å¾…æ•°æ®...")

# å¯åŠ¨æ¥æ”¶çº¿ç¨‹
threading.Thread(target=receive_frames, daemon=True).start()
threading.Thread(target=receive_summary, daemon=True).start()
q_text = "Focus on the historical texts and image frames. Identify any abnormalities in the following content and provide a reason if any are found."

last_frame_hash = None
last_summary_hash = None

# ä¸»å¤„ç†å¾ªç¯
while True:
    time.sleep(0.1)  # æ§åˆ¶ä¸»å¾ªç¯é€Ÿç‡ï¼Œé¿å…ç©ºè·‘å ç”¨ CPU

    with frame_lock:
        if len(frame_buffer) < 8:
            continue
        frame_list = [f.copy() for f in frame_buffer]  # å®‰å…¨å¤åˆ¶æ¯å¸§å›¾åƒ
        current_video = current_video_info['filename']

    with summary_lock:
        summary = latest_summary

    if summary is None:
        continue
    
    
    current_frame_hash = hash_frames(frame_list)
    current_summary_hash = hash_text(summary)

    if current_frame_hash == last_frame_hash and current_summary_hash == last_summary_hash:
        continue  # æ— æ›´æ–°ï¼Œè·³è¿‡å¤„ç†

    # æ›´æ–°è®°å½•
    last_frame_hash = current_frame_hash
    last_summary_hash = current_summary_hash

    print(f"å½“å‰å¤„ç†è§†é¢‘: {current_video}")  # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡®

    # æ¨¡å‹æ¨ç†
    result = build_qaclip(model_with_lora2, processor, q_text, frame_list, summary)
    
    # è¾“å‡ºåˆ°æ§åˆ¶å°
    print("Result:", result)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„æ–‡ä»¶å
    output_file = output_dir / f"{current_video}_{timestamp}.txt"
    
    # ä¿å­˜ç»“æœ
    with open(output_file, 'a', encoding='utf-8') as f:
        f.write(f"time:{time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"video_file: {current_video_info['filename']}\n")
        f.write(f"Summary: {summary}\n")
        f.write(f"\nAnalysis Result: {result}\n")
        f.write("-" * 50 + "\n\n")
    
    print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    
    try:
        publisher.send_json({
        "category": [result],  # ä¿æŒä¸ç½‘é¡µç«¯ä¸€è‡´çš„æ ¼å¼
        "type": "clip",            # æ·»åŠ ç±»å‹æ ‡è¯†
        "timestamp": time.time()    # æ·»åŠ æ—¶é—´æˆ³
    })
        print("âœ… åˆ†æç»“æœå·²å‘å¸ƒåˆ° WebSocket æœåŠ¡å™¨")
    except Exception as e:
        print(f"âŒ å‘å¸ƒåˆ†æç»“æœæ—¶å‡ºé”™: {e}")
